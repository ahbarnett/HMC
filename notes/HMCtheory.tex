\documentclass[10pt]{article}
\oddsidemargin = 0.2in
\topmargin = -0.5in
\textwidth 6in
\textheight 8.5in

\usepackage{graphicx,bm,hyperref,amssymb,amsmath,amsthm}
\usepackage{algorithmic,xcolor}

% -------------------------------------- macros --------------------------
% general ...
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}} 
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\ba}{\begin{align}} 
\newcommand{\ea}{\end{align}}
\newcommand{\bse}{\begin{subequations}} 
\newcommand{\ese}{\end{subequations}}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand{\bfi}{\begin{figure}}
\newcommand{\efi}{\end{figure}}
\newcommand{\ca}[2]{\caption{#1 \label{#2}}}
\newcommand{\ig}[2]{\includegraphics[#1]{#2}}
\newcommand{\bmp}[1]{\begin{minipage}{#1}}
\newcommand{\emp}{\end{minipage}}
\newcommand{\pig}[2]{\bmp{#1}\includegraphics[width=#1]{#2}\emp} % mp-fig, nogap
\newcommand{\bp}{\begin{proof}}
\newcommand{\ep}{\end{proof}}
\newcommand{\ie}{{\it i.e.\ }}
\newcommand{\eg}{{\it e.g.\ }}
\newcommand{\etal}{{\it et al.\ }}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pdc}[3]{\left. \frac{\partial #1}{\partial #2}\right|_{#3}}
\newcommand{\infint}{\int_{-\infty}^{\infty} \!\!}      % infinite integral
\newcommand{\tbox}[1]{{\mbox{\tiny #1}}}
\newcommand{\mbf}[1]{{\mathbf #1}}
\newcommand{\half}{\mbox{\small $\frac{1}{2}$}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\RR}{\mathbb{R}^2}
\newcommand{\ve}[4]{\left[\begin{array}{r}#1\\#2\\#3\\#4\end{array}\right]}  % 4-col-vec
\newcommand{\vt}[2]{\left[\begin{array}{r}#1\\#2\end{array}\right]} % 2-col-vec
\newcommand{\bigO}{{\mathcal O}}
\newcommand{\qqquad}{\qquad\qquad}
\newcommand{\qqqquad}{\qqquad\qqquad}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\vol}{vol}
\newtheorem{thm}{Theorem}
\newtheorem{cnj}[thm]{Conjecture}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{pro}[thm]{Proposition}
\newtheorem{rmk}[thm]{Remark}
\newtheorem{dfn}[thm]{Definition}
% this work...
\newcommand{\al}{\alpha}
\newcommand{\eps}{\varepsilon}

\begin{document}

\title{Notes on theory of Hamiltonian Monte Carlo and delayed rejection}
% asymptotic expansion?


\author{Alex H. Barnett}
\date{\today}
\maketitle

\begin{abstract}
  Since the rigorous proof of invariance of HMC for the proposal density
  is not simply written
  down anywhere we can find, we summarize it here.
  Measure theory is needed for mathematical rigor, but we avoid
  excessively general notation.
  We do not delve into mixing or convergence rates.
  We also aim to write down our delayed rejection HMC method rigorously.
  It also serves as a tutorial,
  and scratch for our paper.
\end{abstract}



% 11111111111111111111111111111111111111111111111111111111111111111111111
\section{Basics: Markov chains and Metropolis--Hastings}

We consider Markov chains on continuous
state spaces, using measure theory in order to handle the non-absolutely
continuous transition kernels of HMC.
Our notation follows, eg Tierney's papers from the 90s, Geyer's and Kennedy's lecture notes.
We avoid the very abstract framework of Andrieu et al 2021.
Also see graduate-level books such as Hunter \& Nachtergaele, or Stein \& Shakarchi, for basic measure theory.

% bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb
\subsection{The basics in pdf notation}

We consider a continuous state space $S = \R^n$.
A state is a point $x\in S$.
The goal of MCMC is to sample from a given distribution $\pi$ over $S$.
In our applications $\pi$ may be taken to be
{\em absolutely continuous} (AC, with respect to
Lebesgue measure $dx$), meaning that it is described by a nonnegative
density {\em function} (pdf) which without ambiguity we may also call
$\pi: S \to \R_{\ge 0}$.
%Thus $\pi(x)$ is a nonnegative number for every $x\in S$.
The normalization is $\int \pi(x) dx = 1$, although all MCMC methods
discussed can handle unnormalized $\pi$.
All integrals are over $S$ unless otherwise indicated.

A Markov chain is defined by a {\em transition operator} $K$, which
in the simplest AC case we may also write as a kernel function
$k(x,y)$ giving the pdf
of the next state $y$ conditioned on
the current state $x$, that is, $p(y|x)$.
The convention in probability is that
operators act from the left
(the opposite of that for usual operators in applied math),
so, such as kernel acts on a density $\pi$ as an integral operator
\be
(\pi K)(y) := \int \pi(x) k(x,y) dx~.
\label{piK}
\ee
A mnemonic for the kernel indices is $k$(initial,final), which is backwards
from the usual in integral equations.
$k(x,\cdot)$ is normalized (the Markov property), ie,
\be
\int k(x,y) dy = 1~, \qquad \forall x\in S~.
\label{knorm}
\ee
Then $\pi K$ is also normalized:
$\int (\pi K)(y) dy = \int\int \pi(x) k(x,y) dx dy = \int \pi(x) [ \int k(x,y) dy] dx = \int \pi(x) dx$,
where swapping the order of integration
is justified by Fubini's theorem because $\pi(x)k(x,y)$ is integrable
over $dx dy$.

Invariance (stationarity) of a pdf $\pi$ with respect to the kernel $K$
is then $\pi K = \pi$ as pdfs, ie
\be
\int \pi(x) k(x,y) dx = \pi(y)~, \qquad \forall y\in S
\label{inv}
\ee
Detailed balance (DB, also called ``reversibility'') for a kernel
(or ``chain'')
is the condition
\be
\pi(x) k(x,y) = \pi(y) k(y,x)~, \qquad \forall x,y\in S
\qquad \mbox{(DB)}
\label{db}
\ee
The proof that DB implies invariance is just by integration
of \eqref{db} with respect to $dy$, then using \eqref{knorm}.

M-H involves a {\em proposal} kernel $q(x,y)$, which we assume is AC for now,
and is also normalized so $\int q(x,y) dy = 1$ for all $x\in S$.
Starting at $x$, the M-H step is: draw $y$ from $q(x,y)$ then
accept with probability $\al(x,y)$ in which case the new $x$ is $y$,
otherwise the new $x$ is $x$. We could introduce chain notation
$x_t$, $t=0,1,\dots$ but little would be gained at this point.
The standard M-H acceptance formula is
\be
\al(x,y) = \min\left(\frac{\pi(y) q(y,x)}{\pi(x) q(x,y)}, 1 \right)
~, \qquad x,y\in S~,
\label{al}
\ee
which obeys
\be
\pi(x) q(x,y) \al(x,y) = \pi(y) q(y,x) \al(y,x)
~, \qquad x,y\in S~.
\label{alrat}
\ee
Note that \eqref{alrat} is more general than \eqref{al} since it allows
for $q=0$ for some $(x,y)$.
It is easy to show that \eqref{alrat} implies \eqref{db} for the
case of $y\neq x$, since in that case $k(x,y) = p(y|x) = q(x,y)\al(x,y)$,
and substituting into \eqref{db} and using \eqref{alrat} shows
\eqref{db} holds for $y\neq x$.
However, we have not proved DB, since what is the meaning of $k(x,x)$,
which is infinite?
What is the meaning of the pointwise equality \eqref{db} for $y=x$ ?
These issues do not arise in the discrete state space $S$ case, only the continous one.
This motivates definitions using measures (distributions), a generalization of density functions, as in the next section.

However, as a warm-up, one does not need measures to prove that M-H is merely
$\pi$ invariant, avoiding DB, as follows.
\begin{pro}
  Let $q(x,.)$ be an AC proposal density, then M-H with acceptance probability
  \eqref{al} has $\pi$ an invariant density.
  \label{p:mhinv}
\end{pro}
\begin{proof}
  At the current state $x$, the probability of rejection
  (called $s$ in Andrieu) is
  \be
  r(x) = \int q(x,z) [1-\al(x,z)] dz  = 1 - \int q(x,z) \al(x,z) dz~;
  \label{rej}
  \ee
  here $z$ is a dummy variable.
  The action of an M-H step on $\pi$ is a mixture of an $\al$-weighted proposal
  and the rejection (no change, $y=x$),
  resulting in the final density as a function of final state $y$,
  \bea
  \int \pi(x) q(x,y)\al(x,y) dx + \pi(y) r(y)
  &=& \int \pi(y) q(y,x)\al(y,x) dx + \pi(y) r(y)
  \nonumber \\
  &=& \pi(y) \int q(y,x)\al(y,x) dx + \pi(y) r(y)
  \nonumber \\
  &=& \pi(y) [1-r(y)] + \pi(y) r(y) \; = \; \pi(y)
  \nonumber
\eea
where we applied \eqref{alrat} to the integrand,
and to get to the third line used \eqref{rej}.
Comparing \eqref{inv} completes the proof.
\end{proof}

\begin{rmk}
Any form of $\al(x,y)$ that satisfies \eqref{alrat} is valid
in the above; however, \eqref{al} is the choice that is most efficient
in the sense that any other has higher probability of rejection.
This is simply because, for each $x,y\in S$,
either $\al(x,y)$ or $\al(y,x)$ is 1, the largest
allowed value for a probability.
\end{rmk}


% ddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddd
\subsection{Distributions and measure theory notation}

M-H, and in particular HMC,
involves transition kernels for which $k(x,\cdot)$
is not AC.
One extension of the function notation that can handle
this is to use $\delta$, the Dirac delta distribution, or unit point mass, defined by
$$
\delta(x) = 0, \quad x\neq 0, \qquad \int \delta(x) dx = 1~.
$$
For any function $f$ continuous at $0$, we have the sifting property
$\int f(x) \delta(x) dx = f(0)$.
The transformation rules are, in 1D ($n=1$), with $f:\R \to \R$ a
differentiable function,
\bea
\delta(ax) &=& |a|^{-1} \delta(x)~, \qquad a \in \R, \; a\neq 0~,
\\
\delta(f(x)) &=& \sum_{z: f(z) = 0} |f'(z)|^{-1} \delta(x-z)~.
\eea
Of course the latter is inapplicable when the RHS is not defined.
In general dimension ($n\ge 1$), where $F:S\to S$ is a differentiable map,
\be
\delta(F(x)) = \sum_{z: F(z) = 0} |\det DF(z)|^{-1} \delta(x-z)~,
\ee
where $DF$ is the $n\times n$ Jacobean derivative matrix $\partial_{x_j} F_i$,
and $0$ is the origin in $\R^n$.
By replacing $F(x)$ by $F(x)-b$, for some constant translation $b\in \R^n$,
\be
\delta(F(x)-b) = \sum_{z: F(z) = b} |\det DF(z)|^{-1} \delta(x-z)~.
\label{deltarule}
\ee
Here $z$ sums over the preimages of $b$.
Subscript notation is also used:
$\delta_x(y) = \delta(x-y) = \delta(y-x) = \delta_y(x)$.

For example, in the above case of M-H with an AC proposal density
$q(x,y)$, the kernel for the M-H step
is a mixture of an AC pdf and a (rejected) point mass at $y=x$,
\be
k(x,y) = q(x,y)\al(x,y) + \delta_x(y) r(x)~,
\label{mhker}
\ee
which one may check obeys $\int k(x,y) dy=1$ for all $x$,
recalling \eqref{rej}.
See Tierney '98, Kennedy Sec.~8.2.
Note, however, that not all non-AC measures are sums of point masses;
there may be distributions on intermediate dimension subsets, fractals, etc.

One must generalize the notation to measures rather than
functions.
To recap this,
measures are defined over $(S,\cal S)$, where $\cal S$ is a
``$\sigma$-field over $S$''.
Loosely speaking, $\cal S$ is the set of all measurable
subsets of $S$ (see any textbook on measure theory).
A measure $\pi$ on $(S,\cal S)$ is defined by
$\pi(B)$, i.e., probability of being in $B$,
for all (measurable) sets $B\subset S$ (strictly, $B\in \cal S$).
The normalization is $\pi(S) = 1$.
Only when a measure $\pi$ is AC
can we write it as a pdf $\pi(x)$
(here we overload the notation; sometimes a distinct symbol is used),
in which case for any $B\in\cal S$,
$$
\pi(B) = \int_B \pi(x) dx~.
$$
Here $\pi(x) dx$ may be thought of as a measure, also written
$\pi(dx)$.
Lebesgue measure $dx$ is a special case of a measure with unit density function.
Equivalent notations (see Kennedy notes p.~89) include in the measure case
$$
\pi(B) = \int_B d\pi = \int_B d\pi(x) = \int_B \pi(dx) ~.
$$
Two measures $\pi$ and $\mu$ are equal if
$$
\pi(B) = \mu(B) \qquad \forall B\in\cal S~;
$$
note that this is a {\em weak} definition of equality.
This is sometimes summarized by $\pi(dx) = \mu(dx)$, which
is useful since it makes explicit an independent variable $x$;
however it is somewhat imprecise, being analogous to
writing ``$f(x) = g(x)$'' to express equality of $f$ and $g$ as functions.

A transition (Markov) kernel $K(x,\cdot)$ can be considered a measure
that depends on the parameter $x\in S$, namely the initial point.
The normalization is $K(x,S)=1$ for any $x\in S$.
Then $K(x,B) = p(y\in B | x)$ is the conditional probability of ending in
$B$ after one step, when starting in at $x$.
The action of a kernel $K$ on a measure $\pi$ is
(compare \eqref{piK}),
for any ``test'' set $B \in \cal S$,
$$
(\pi K)(B) = \int_S \pi(dx) K(x,B)~.
$$
The measure version of \eqref{inv} is as follows (Kennedy (7.33)).
\begin{dfn}[Invariance for measures]
  A measure $\pi$ is invariant under the Markov kernel $K$ if
  $\pi K = \pi$, as measures, that is
  \be
  \int \pi(dx) K(x,B) = \pi(B) ~, \qquad \forall B \in \cal S~.
  \ee
  \label{d:invm}
\end{dfn}
The direct proof of M-H invariance for \eqref{mhker} (Prop.~\ref{p:mhinv})
in this sense of invariance is
done in, eg, Tierney '94 Sec.~2.3.1 and notes by Geyer, Kennedy.
They all avoid the route via DB.

The measure (weak) sense of \eqref{db}
states: the probability of being in set $A$ then going to
set $B$ is the same as the other way around, as follows.
\begin{dfn}[Detailed balance for measures]
  A kernel $K$ has detailed balance (reversible) with respect to
  a measure $\pi$ if
  \be
  \int_A \pi(dx) K(x,B) = \int_B \pi(dx) K(x,A) \qquad \forall A,B \in \cal S~.
  \label{dbm}
  \ee
  \label{d:dbm}
\end{dfn}
A summary of this is $\pi(dx) K(x,dy) = \pi(dy) K(y,dx)$ as measures
on $\cal S \otimes \cal S$; this needs the standard idea of a product measure.
A less abstract way to write it is to return to weak equality of
distributions,
$$
\int_B \int_A \pi(x) k(x,y) dy dx =
\int_B \int_A \pi(y) k(y,x) dx dy \qquad \forall A,B \subset S~,
$$
using $k(x,y)dy$ to represent the measure $K(x,dy)$.

\begin{pro}
  With respect to a measure $\pi$, detailed balance of a Markov operator $K$ implies invariance.
  \label{p:dbim}
\end{pro}
\begin{proof}
  Choose $A=S$ in \eqref{dbm}. The left hand side is $(\pi K)(B)$.
  On the right hand side $K(x,S)=1$ for all $x$ is the Markov property,
  leaving $\pi(B)$. This holds for any $B\in \cal S$.
\end{proof}


% QQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQ
\subsection{M-H with proposals that are measures}

Tierney '98 and Andrieu prove that M-H with acceptance a generalization of
\eqref{al}
obeys DB in the measure sense; they are too abstract for our purposes.
% I don't get the analysis in Tierney '98, and certainly not Andrieu.
We now instead show the same in a simpler way.

We need M-H general measure proposal $Q(x,dy)$, which might not
be represented by an AC kernel $q(x,y)dy$ for any function $q(x,\cdot)$.
In this case the $\al$ condition \eqref{alrat} is meaningless and must be replaced by
\be
\int_{x\in A} \int_{y \in B} \pi(dx) Q(x,dy) \al(x,y) = \int_{x\in A} \int_{y\in B} \pi(dy) Q(y,dx) \al(y,x)~, \qquad \forall A,B \in \cal S~.
\label{alratm}
\ee
Note that the $dy$ notation is unavoidable because
$\al$ depends on $(x,y)$ and thus modifies $Q$ by pointwise multiplication;
one cannot use the set notation of \eqref{dbm}.
Here we take $\al$ as a function of $(x,y)$, which thus may safely multiply
measures pointwise in both variables.
% without Radon-Nykodim deriv, yuk
% $\al$ need only be defined on the support.
\eqref{alratm} can be summarized by an equality of
measures on $\cal S \otimes \cal S$,
\be
\pi(dx) Q(x,dy) \al(x,y) = \pi(dy) Q(y,dx) \al(y,x)~,
\label{alratmi}
\ee
in other words that the left hand side measure of \eqref{alratmi} is symmetric
with respect to $x\leftrightarrow y$.
\footnote{Andrieu states this way more abstractly as: the ratio $\al(y,x)/\al(x,y)$
is the Radon--Nikodym derivative of $\pi(dx) Q(x,dy)$ and its
$x\leftrightarrow y$ transpose.}


The rejection probability is, for each $x\in S$,
\be
r(x) = 1 - \int Q(x,dz) \al(x,z)~,
\label{rejm}
\ee
and, in terms of this, the M-H transition measure (with respect to $y$) is
\be
K(x,dy) = Q(x,dy) \al(x,y) + r(x) \delta_x(dy)~,
\label{mhkerm}
\ee
or (Kennedy (8.7)),
\be
K(x,B) = \int_B Q(x,dy) \al(x,y) + r(x) 1_{x\in B}~, \qquad \forall B \in \cal S
~.
\label{mhkerB}
\ee

\begin{lem}[M-H for proposals that are measures]
  For any acceptance probability function $\al(x,y)$ obeying \eqref{alratm},
  M-H with a proposal measure $Q(x,\cdot)$ has detailed balance with respect to the measure $\pi$.
  \label{l:mh}
\end{lem}
\begin{proof}
  Let $A,B\in \cal S$.
  Inserting \eqref{mhkerB} into the LHS of \eqref{dbm}
  gives
  $$
  \int_A \pi(dx) K(x,B) = \int_{x\in A} \pi(dx) \int_{y\in B} Q(x,dy) \al(x,y)
  + \int_{A\cap B} \pi(dx) r(x)
  $$
  Instead inserting into the RHS gives
  $$
  \int_B \pi(dx) K(x,A) = \int_{x\in B} \pi(dx) \int_{y\in A} Q(x,dy) \al(x,y)
  + \int_{A\cap B} \pi(dx) r(x)
  $$
  The second terms are equal.
  By \eqref{alratm} (swapping the roles $x\leftrightarrow y$) the first terms are equal,
  so \eqref{dbm} holds for all $A,B$.
\end{proof}

Combining with Prop.~\ref{p:dbim} shows that for general proposal measures,
M-H with $\al$ condition \eqref{alratm} has the correct $\pi$-invariance.



% FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\section{M-H with a deterministic proposal from a map}
% Q symm needed?

One component of Hamiltonian MC is an M-H step using a proposal measure defined by a map.
Let $F: S\to S$ be a deterministic smooth map (this will be given by, eg,
leapfrog steps).
Let the proposal measure $Q_F(x,\cdot)$ be defined by
$$
Q_F(x,B) = 1_{F(x)\in B} \qquad \forall B \in \cal S~,
$$
which simply places a unit point mass at $F(x)$.
This can also be written
$$
Q_F(x,dy) = \delta_{F(x)}(dy) = \delta(y - F(x)) dy ~.
$$
\begin{rmk} Acting this proposal directly as a Markov kernel (ie, always accepting) onto a measure $\pi$ gives $\pi Q_F$, the so-called ``pushforward'',
  defined by
$$
(\pi Q_F)(B) = \int \pi(dx) Q_F(x,B) = \pi(F^{-1}(B)) \qquad \forall B \in \cal S~,
$$
and often denoted by $F_\ast \pi$, or by $\pi^F$ (Andrieu).
\end{rmk}

\begin{dfn}[Involution]
  A map $F:S\to S$ is an involution if $F^{-1} = F$,
  that is, $F^2=I$ where $I$ is the identity.
\end{dfn}
Andrieu, building on Tierney '98 p.4, emphasizes involutions,
although this is confused for this reader with the $x\leftrightarrow y$ flip
(see Andrieu eq.~(4)).
%Sohl--Dickstein uses them 

\begin{dfn}[Liouville]
  A map $F:S\to S$ is volume-preserving if it preserves Lebesgue measure,
  that is,
  $$
  \int_B dx = \int_{F(B)} dx \qquad \forall B \subset S~,
  $$
  where $F(B):=\{F(x): \, x\in B\}$ is the image of the set $B$.
\end{dfn}
Volume preservation is equivalent to $F$ having everywhere unit Jacobean,
$|\det DF(x)| = 1$ for all $x\in S$.
% need reference

\begin{lem}[M-H using a deterministic volume-preserving involution]
  Let $\pi$ be an AC target density.
  Let $F$ be a volume-preserving involution.
  Then M-H with the proposal measure $Q_F$, and acceptance
  probability $\al$ obeying
  \be
  \pi(x) \al(x,y) = \pi(y) \al(y,x)   \qquad \forall x,y \in S
  \label{alrats}
  \ee
  has DB with respect to $\pi$, and $\pi$ as an invariant measure.
  \label{l:mhmap}
\end{lem}
\begin{proof}
  The conditions on $F$ render $Q_F$ symmetric (with respect to Lebesgue)
  as a measure on $\cal S \otimes \cal S$:
  $$
  dx Q_F(x,dy) = \delta(F(x)-y) dxdy =
  |\det DF(F^{-1}(y))|^{-1} \delta(x - F^{-1}(y)) dxdy =
  \delta(x-F(y)) dxdy =
  dy Q_F(y,dx)~.
  $$
  Here, the 2nd step used the transformation rule \eqref{deltarule}, there being
  only one term in the sum because $F$ is bijective, and
  the next step uses the involution and unit Jacobean.
  Combining this and \eqref{alrats} shows that \eqref{alratm} holds,
  so that DB holds by Lemma~\ref{l:mh}.
  Then invariance follows by Prop.~\ref{p:dbim}.
\end{proof}

There is probably an integral change of variable version of this proof too,
which might be even simpler.
In any case, this formalizes verbal arguments such as those of Neal '11 using infinitesimal
volumes.


% HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH
\section{Hamiltonian Monte Carlo}

Here we use the above tools to prove that HMC has the correct invariant pdf.

The goal is to sample from a pdf $\pi(q)$ over a $d$-dimensional
Euclidean state space $q\in \R^d$. ($\pi$ is assumed AC.)
The MCMC state space is augmented to $x = (q,p)$ where $q\in \R^d$ is now interpreted as position and $p\in\R^d$ as momentum. Thus $S = \R^{2d}$, which
is called phase space.
In the simplest (scalar mass) case the Hamiltonian $H:S \to \R$ is
$$
H(x) = H(q,p) = U(q) + \frac{1}{2}\|p\|^2
$$
where the potential is constructed as $U(q) = -\log \pi(q)$, that is, $\pi(q) = e^{-U(q)}$,
and $\|.\|$ is the 2-norm in $\R^d$.
This assumes $\pi$ is everywhere positive.
Since $H$ is the sum of terms in $q$ and terms in $p$,
Gibbs (aka Boltzmann) measure $\gamma$, defined as follows, is separable:
\be
\gamma(x) := Z^{-1} e^{-H(x)} = Z^{-1} e^{-U(q)} e^{-\|p\|^2/2} = Z^{-1} \pi(q) e^{-\|p\|^2/2}~,
\label{Gibbs}
\ee
where $Z>0$ is some normalizing constant.
Thus the $q$-marginal of $\gamma$ is the desired $\pi$.
The goal is to show that the steps in HMC are $\gamma$-invariant.

Hamiltonian flow with respect to a continuous time variable $t$ is then
(where dot means $\partial_t$)
\be
\left\{\begin{array}{lllll}\dot q &=& \nabla_p H(q(t),p(t)) &=& p(t) \\
\dot p &=& -\nabla_q H(q(t),p(t)) &=& -\nabla U(q(t))
\end{array}\right.
\label{flow}
\ee
Since HMC does not use this flow, we skip any proofs about it for now
(it is $H$-preserving, volume-preserving, symplectic, time-reversible, etc).

\begin{dfn}[Shear]
  Any map on $\R^{2d}$ of the form
  $(q,p) \mapsto (q + G(p),p)$, or
  $(q,p) \mapsto (q, p +G(q))$,
  where $G:\R^d\to \R^d$ is some differentiable map, is called a {\em shear}.
\end{dfn}
\begin{pro}
  Any shear is volume-preserving.
  \label{p:shear}
\end{pro}
\begin{proof}
  Let $F$ be a shear.
  Computing its Jacobean with $d\times d$ blocks,
  $DF = \left[\begin{smallmatrix}I_d&DG\\0 &I_d \end{smallmatrix}\right]$, or
    $DF = \left[\begin{smallmatrix}I_d &0\\DG &I_d \end{smallmatrix}\right]$.
  In either case $\det DF \equiv 1$.
\end{proof}


Fixing a timestep $\eps>0$, the leapfrog (Verlet) map $L$,
acting on an ``initial'' state $x_k:=(q_k,p_k)$,
to give a ``final'' state $(q_{k+1},p_{k+1}) = L(q_k,p_k)$,
comprises three sequential steps:
\ben
\item $p'     \;\leftarrow\; p_k - \frac{\eps}{2} \nabla U(q_k)$
\item $q_{k+1} \;\leftarrow\; q_k + \eps p'$
\item $p_{k+1} \;\leftarrow\; p' - \frac{\eps}{2} \nabla U(q_{k+1})$
\een
Let $P$ be the momentum-flip operator
$$
P(q,p) := (q,-p)~.
$$
\begin{lem}
  Let $n\in\{0,1,\dots\}$.
  The map $L^n P$ is a volume-preserving involution.
  (Here, as above, we compose operators to the right, so this
  means $L^n$ followed by $P$, although it happens not to
  matter here.)
  \label{l:LnP}
\end{lem}
\begin{proof}
  $L$ is the composition of three steps each of which is a shear, and thus
  volume-preserving by Prop.~\ref{p:shear}. $P$ is obviously volume-preserving.
  Thus $L^nP$ is volume-preserving.
  $L$ is time-reversible in the sense that, if $L(q_k,p_k)=(q_{k+1},p_{k+1})$,
  one may verify $L(q_{k+1},-p_{k+1})=(q_k,-p_k)$ by
  checking the three steps in reverse order ($p'$ is negated relative
  to its forward value).
  The same is true for $L^n$, by so reversing each leapfrog.
  Stating this algebraically, $PL^n P = L^{-n}$. Using $P=P^{-1}$ and
  rearranging, $(L^nP)^2 = I$, so $L^n P$ is an involution.
\end{proof}

\begin{rmk}
  $L^n$ happens to be
  an $\bigO(\eps^2)$-accurate approximation to integrating the
  flow \eqref{flow} to time $t = n\eps$,
  although this plays no role in the correctness of HMC.
  There exist other step schemes that are also volume-preserving involutions,
  for instance ``modified FE'' in Neal '11 (but not plain FE = forward Euler),
  which is $\bigO(\eps)$-accurate.
\end{rmk}

Vanilla HMC has algorithm parameters $\eps>0$ the timestep, and $n$ a number of leapfrog steps per trajectory.
A single ``trajectory'' composes two steps in sequence:
\ben
\item Gibbs-sample (randomize or partially so) $p$ in a way that leaves the
  Gaussian marginal $e^{-\|p\|^2/2}$ invariant (without affecting $q$).
\item Perform one M-H MCMC step to the state $x=(q,p)$
  using the deterministic proposal $Q_F$ generated by the map $F = L^n P$,
  with acceptance probability $\al$ obeying \eqref{alrats},
  such as the standard
  $$
  \al(x,y) = \min\left(\frac{\pi(y)}{\pi(x)}, 1\right)~.
  $$
  This either changes $(q,p)=x$ to $y$, or rejects leaving it unchanged.
\een
This {\em pair} of steps may then be repeated to generate a Markov chain.
Note that this is not a Markov chain generated by any
M-H rule: the Markov operator for the chain is a Gibbs move plus a M-H update.

\begin{thm}[Invariant pdf of HMC]
  Let $\pi$ be a continuous positive pdf.
  The Markov operator defined by one trajectory of vanilla HMC as
  described above has $\gamma$,
  defined by \eqref{Gibbs}, as an invariant pdf.
  \label{t:hmc}
\end{thm}
\begin{proof}
  It is sufficient to show that each step in the pair is $\gamma$-invariant.
  This holds for step 1 (a Gibbs update) since it preserves the conditional
  over $p$, which is identical at each fixed $q$, while leaving $q$ unaffected.
  It holds for step 2 (one deterministic M-H step) since by
  Lemma~\ref{l:LnP}, $F=L^n P$
  is a volume-preserving involution, so one can apply Lemma~\ref{l:mhmap}.
\end{proof}

We have not discussed uniqueness, or mixing rates, at all.
However, assuming that the Markov chain {\em converges} to the
unique invariant pdf $\gamma$,
the fact that marginalizing \eqref{Gibbs} over $p$ leaves $\pi(q)$
immediately give the following.

\begin{cor}
  Assuming a converged chain,
  vanilla HMC generates a sequence of $q$ values drawn from $\pi$.
\end{cor}
Note that if the Gibbs move (step 1) does not fully randomize $p$,
then one cannot write vanilla HMC as {\em any} Markov chain in $q$ alone:
$p$ has a memory that destroys the Markov property in $q$.



\begin{rmk}
  The point of the Gibbs moves alternating with the M-H moves
  is to increase mixing with respect to
  the value of $H$. A chain of M-H moves alone, while
  also $\gamma$-invariant would approximately
  (to accuracy $\bigO(\eps^2)$) conserve $H$, because \eqref{flow} conserves
  $H$ exactly.
\end{rmk}

\begin{rmk}
  In what sense does the overall Markov step (Gibbs followed by M-H) for vanilla HMC obey DB?
  It appears not to, since while the Gibbs sampler and the M-H step
  each separately obey DB, they do not commute as operators.
  As Geyer reminds, DB is equiv to self-adjointness under the $L^2_\pi$ metric,
  and given $A^\ast=A$ and $B^\ast=B$,
  $(AB)^\ast = BA \neq AB$ unless they commute.
  So I am confused by Solh-Dickstein's discussion of HMC as having DB\ldots
\end{rmk}

Questions:
\ben
\item Understand DB in remark above.
\item What goes wrong in the proof of $Q$ a symmetric product measure
  if $F=L^n$ which is not an involution?
\item Understand Campos \& Sanz-Serna.
\een
  



% DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD
\section{Delayed rejection}

*** To do





% BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB
\bibliographystyle{abbrv}
\bibliography{localrefs}
\end{document}

