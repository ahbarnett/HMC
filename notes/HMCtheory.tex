\documentclass[10pt]{article}
\oddsidemargin = 0.2in
\topmargin = -0.5in
\textwidth 6in
\textheight 8.5in

\usepackage{graphicx,bm,hyperref,amssymb,amsmath,amsthm}
\usepackage{algorithmic,xcolor}

% -------------------------------------- macros --------------------------
% general ...
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}} 
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\ba}{\begin{align}} 
\newcommand{\ea}{\end{align}}
\newcommand{\bse}{\begin{subequations}} 
\newcommand{\ese}{\end{subequations}}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand{\bfi}{\begin{figure}}
\newcommand{\efi}{\end{figure}}
\newcommand{\ca}[2]{\caption{#1 \label{#2}}}
\newcommand{\ig}[2]{\includegraphics[#1]{#2}}
\newcommand{\bmp}[1]{\begin{minipage}{#1}}
\newcommand{\emp}{\end{minipage}}
\newcommand{\pig}[2]{\bmp{#1}\includegraphics[width=#1]{#2}\emp} % mp-fig, nogap
\newcommand{\bp}{\begin{proof}}
\newcommand{\ep}{\end{proof}}
\newcommand{\ie}{{\it i.e.\ }}
\newcommand{\eg}{{\it e.g.\ }}
\newcommand{\etal}{{\it et al.\ }}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pdc}[3]{\left. \frac{\partial #1}{\partial #2}\right|_{#3}}
\newcommand{\infint}{\int_{-\infty}^{\infty} \!\!}      % infinite integral
\newcommand{\tbox}[1]{{\mbox{\tiny #1}}}
\newcommand{\mbf}[1]{{\mathbf #1}}
\newcommand{\half}{\mbox{\small $\frac{1}{2}$}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\RR}{\mathbb{R}^2}
\newcommand{\ve}[4]{\left[\begin{array}{r}#1\\#2\\#3\\#4\end{array}\right]}  % 4-col-vec
\newcommand{\vt}[2]{\left[\begin{array}{r}#1\\#2\end{array}\right]} % 2-col-vec
\newcommand{\bigO}{{\mathcal O}}
\newcommand{\qqquad}{\qquad\qquad}
\newcommand{\qqqquad}{\qqquad\qqquad}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\vol}{vol}
\newtheorem{thm}{Theorem}
\newtheorem{cnj}[thm]{Conjecture}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{pro}[thm]{Proposition}
\newtheorem{rmk}[thm]{Remark}
\newtheorem{dfn}[thm]{Definition}
% this work...
\newcommand{\al}{\alpha}


\begin{document}

\title{Notes on theory of Hamiltonian Monte Carlo and delayed rejection}
% asymptotic expansion?


\author{Alex H. Barnett}
\date{\today}
\maketitle

\begin{abstract}
  Since the rigorous proof of invariance of HMC for the proposal density
  is not simply written
  down anywhere we can find, we summarize it here.
  Measure theory is needed for mathematical rigor, but we avoid
  excessively general notation.
  We do not delve into mixing or convergence rates.
  We also aim to write down our delayed rejection HMC method rigorously.
  It also serves as a tutorial,
  and scratch for our paper.
\end{abstract}



\section{Basics: Markov chains and Metropolis--Hastings in general}

We consider Markov chains on continuous
state spaces, using measure theory in order to handle the non-absolutely
continuous transition kernels of HMC.
Our notation follows, eg Tierney's papers from the 90s, Geyer's and Kennedy's lecture notes.
We avoid the very abstract framework of Andrieu et al 2021.
Also see graduate-level books such as {\em Applied Analysis} by Hunter \& Nachtergaele for basic measure theory.

\subsection{The basics in pdf notation}

We consider a continuous state space $S = \R^n$.
A state is a point $x\in S$.
All integrals are over $S$ unless otherwise indicated.
The goal of MCMC is to sample from a given distribution $\pi$ over $S$.
In our applications $\pi$ may be taken to be
{\em absolutely continuous} (AC, with respect to
Lebesgue measure $dx$), meaning that it is described by a nonnegative
density {\em function} (pdf) which without ambiguity we may also call
$\pi: S \to \R_{\ge 0}$.
Thus $\pi(x)$ is a nonnegative number for every $x\in S$.
The normalization is $\int \pi(x) dx = 1$.
A Markov chain is defined by a {\em transition operator} $K$, which
in the simplest AC case we may also write as a kernel function
$k(x,y)$ giving the pdf
of the next state $y$ conditioned on
the current state $x$, that is, $p(y|x)$.
The convention in probability is that
operators act from the left
(the opposite of that for usual operators in applied math),
so, such as kernel acts on a density $\pi$ as in integral kernel
\be
(\pi K)(y) := \int \pi(x) k(x,y) dx~.
\label{piK}
\ee
A mnemonic for the kernel indices is $k$(initial,final), which is backwards
from the usual in integral equations.
By normalization of $k(x,\cdot)$,
\be
\int k(x,y) dy = 1~, \qquad \forall x\in S~.
\label{knorm}
\ee
Inserting this into
$\int (\pi K)(dy) dy = \int\int \pi(x) k(x,y) dx dy$ after swapping
the order of integration, proves that $\pi K$ is also normalized.

Invariance (stationarity) of a pdf $\pi$ with respect to the kernel $K$
is then $\pi K = \pi$ as pdfs, ie
\be
\int \pi(x) k(x,y) dx = \pi(y)~, \qquad \forall y\in S
\label{inv}
\ee
Detailed balance (DB, also called ``reversibility'') for a kernel
(or ``chain'')
is the condition
\be
\pi(x) k(x,y) = \pi(y) k(y,x)~, \qquad \forall x,y\in S
\qquad \mbox{(DB)}
\label{db}
\ee
The proof that DB implies invariance is just by integration
of \eqref{db} with respect to $dy$, then using \eqref{knorm}.

M-H involves a proposal kernel $q(x,y)$, which we assume is AC for now,
and is normalized so $\int q(x,y) dy = 1$, for all $x\in S$.
Starting at $x$, the M-H step is: draw $y$ from $q(x,y)$ then
accept with probability $\al(x,y)$ in which case the new $x$ is $y$,
otherwise the new $x$ is $x$. We could introduce chain notation
$x_t$, $t=0,1,\dots$ but little would be gained at this point.
The M-H acceptance formula is
\be
\al(x,y) = \min\left(\frac{\pi(y) q(y,x)}{\pi(x) q(x,y)}, 1 \right)
~, \qquad x,y\in S
\label{al}
\ee
which obeys
\be
\pi(x) q(x,y) \al(x,y) = \pi(y) q(y,x) \al(y,x)
~, \qquad x,y\in S
\label{alrat}
\ee
Note that \eqref{alrat} is more general than \eqref{al} since it allows
for vanishing of $q(x,y)$ for certain $(x,y)$.
It is easy to show that \eqref{alrat} implies \eqref{db} for the
case of $y\neq x$, since in that case $k(x,y) = p(y|x) = q(x,y)\al(x,y)$,
and substituting into \eqref{db} and using \eqref{alrat} shows
\eqref{db} holds for $y\neq x$.
However, we have not proved DB, since what is the meaning of $k(x,x)$,
which is infinite?
What is the meaning of the pointwise equality \eqref{db} for $y=x$ ?
This motivates definitions using measures (distributions), a generalization of density functions, as in the next section.

However, as a warm-up, one does not need measures to prove that M-H is merely
$\pi$ invariant, avoiding DB, as follows.
\begin{pro}
  Let $q(x,.)$ be an AC proposal density, then M-H with acceptance probability
  \eqref{al} has $\pi$ an invariant density.
  \label{p:mhinv}
\end{pro}
\begin{proof}
  At the current state $x$, the probability of rejection
  (called $s$ in Andrieu) is
  \be
  r(x) = \int q(x,z) [1-\al(x,z)] dz  = 1 - \int q(x,z) \al(x,z) dz~;
  \label{rej}
  \ee
  here $z$ is a dummy variable.
  The action of an M-H step on $\pi$ is a mixture of an $\al$-weighted proposal
  and the rejection (no change, $y=x$),
  resulting in the final density as a function of final state $y$,
  \bea
  \int \pi(x) q(x,y)\al(x,y) dx + \pi(y) r(y)
  &=& \int \pi(y) q(y,x)\al(y,x) dx + \pi(y) r(y)
  \nonumber \\
  &=& \pi(y) \int q(y,x)\al(y,x) dx + \pi(y) r(y)
  \nonumber \\
  &=& \pi(y) [1-r(y)] + \pi(y) r(y) \; = \; \pi(y)
  \nonumber
\eea
where we applied \eqref{alrat} to the integrand,
and to get to the third line used \eqref{rej}.
Comparing \eqref{inv} completes the proof.
\end{proof}

\begin{rmk}
Any form of $\al(x,y)$ that satisfies \eqref{alrat} is valid
in the above; however, \eqref{al} is the choice that is most efficient
in the sense that any other has higher probability of rejection.
This is simply because, for each $x,y\in S$,
either $\al(x,y)$ or $\al(y,x)$ is 1, the largest
allowed value for a probability.
\end{rmk}


\subsection{Distributions and measure theory notation}

M-H, and in particular HMC,
involves transition kernels for which $k(x,\cdot)$
is not AC.
One extension of the function notation that can handle
this is to use $\delta$, the Dirac delta distribution, or unit point mass, defined by
$$
\delta(x) = 0, \quad x\neq 0, \qquad \int \delta(x) dx = 1~.
$$
For any continuous function $f:S \to \R$, we have the sifting property
$\int f(x) \delta(x) dx = f(0)$.
The transformation rules are, in 1D ($n=1$),
\bea
\delta(ax) &=& |a|^{-1} \delta(x)~, \qquad a \in \R, \; a\neq 0~,
\\
\delta(F(x)) &=& \sum_{z: f(z) = 0} |F'(z)|^{-1} \delta(x-z)~,
\qquad F\in C^1(\R)~.
\eea
which generalizes to higher dimensions ($n\ge 1$), where $F:S\to S$,
\be
\delta(F(x)) = \sum_{z: F(z) = 0} |\det DF(z)|^{-1} \delta(x-z)~,
\ee
where $DF$ is the $n\times n$ Jacobean derivative matrix of $F$.
The notation $\delta_x(y)$ means $\delta(x-y)$ which also equals $\delta_y(x)$.

For example, in the above case of M-H with an AC proposal density
$q(x,y)$, the kernel for the M-H step
is a mixture of an AC pdf and a (rejected) point mass at $y=x$,
\be
k(x,y) = q(x,y)\al(x,y) + \delta_x(y) r(x)~,
\label{mhker}
\ee
which one may check obeys $\int k(x,y) dy=1$ for all $x$,
recalling \eqref{rej}.
See Tierney '98, Kennedy Sec.~8.2.
Note, however, that not all non-AC measures are sums of point masses;
there may be distributions on intermediate dimension subsets, fractals, etc.

One must generalize the notation to measures rather than
functions.
To recap this,
measures are defined over $(S,\cal S)$, where $\cal S$ is a
``$\sigma$-field over $S$''.
Loosely speaking, $\cal S$ is the set of all measurable
subsets of $S$ (see any textbook on measure theory).
A measure $\pi$ on $(S,\cal S)$ is defined by
$\pi(B)$, i.e., probability of being in $B$,
for all (measurable) sets $B\subset S$ (strictly, $B\in \cal S$).
The normalization is $\pi(S) = 1$.
Only when a measure $\pi$ is AC
can we write it as a pdf $\pi(x)$
(here we overload the notation; sometimes a distinct symbol is used),
in which case for any $B\in\cal S$,
$$
\pi(B) = \int_B \pi(x) dx~.
$$
Here $\pi(x) dx$ may be thought of as a measure, also written
$\pi(dx)$.
Lebesgue measure $dx$ is a special case of a measure with unit density function.
Equivalent notations (see Kennedy notes p.~89) include in the measure case
$$
\pi(B) = \int_B d\pi = \int_B d\pi(x) = \int_B \pi(dx) ~.
$$
Two measures $\pi$ and $\mu$ are equal if
$$
\pi(B) = \mu(B) \qquad \forall B\in\cal S~;
$$
note that this is a {\em weak} definition of equality.
This is sometimes summarized by $\pi(dx) = \mu(dx)$, which
is useful since it makes explicit an independent variable $x$;
however it is somewhat imprecise, being analogous to
writing ``$f(x) = g(x)$'' to express equality of $f$ and $g$ as functions.

A transition (Markov) kernel $K(x,\cdot)$ can be considered a measure
that depends on the parameter $x\in S$, namely the initial point.
The normalization is $K(x,S)=1$ for any $x\in S$.
Then $K(x,B) = p(y\in B | x)$ is the conditional probability of ending in
$B$ after one step, when starting in at $x$.
The action of a kernel $K$ on a measure $\pi$ is
(compare \eqref{piK}),
for any ``test'' set $B \in \cal S$,
$$
(\pi K)(B) = \int_S \pi(dx) K(x,B)~.
$$
The measure version of \eqref{inv} is as follows (Kennedy (7.33)).
\begin{dfn}[Invariance for measures]
  A measure $\pi$ is invariant under the Markov kernel $K$ if
  $\pi K = \pi$, as measures, that is
  \be
  \int \pi(dx) K(x,B) = \pi(B) ~, \qquad \forall B \in \cal S~.
  \ee
  \label{d:invm}
\end{dfn}
The direct proof of M-H invariance for \eqref{mhker} (Prop.~\ref{p:mhinv})
in this sense of invariance is
done in, eg, Tierney '94 Sec.~2.3.1 and notes by Geyer, Kennedy.
They all avoid the route via DB.

The measure (weak) sense of \eqref{db}
states: the probability of being in set $A$ then going to
set $B$ is the same as the other way around, as follows.
\begin{dfn}[Detailed balance for measures]
  A kernel $K$ has detailed balance (reversible) with respect to
  a measure $\pi$ if
  \be
  \int_A \pi(dx) K(x,B) = \int_B \pi(dx) K(x,A) \qquad \forall A,B \in \cal S~.
  \label{dbm}
  \ee
  \label{d:dbm}
\end{dfn}
A summary of this is $\pi(dx) K(x,dy) = \pi(dy) K(y,dx)$ as measures
on $\cal S \otimes \cal S$; this needs the standard idea of a product measure.
A less abstract way to write it is to return to weak equality of
distributions,
$$
\int_B \int_A \pi(x) k(x,y) dy dx =
\int_B \int_A \pi(y) k(y,x) dx dy \qquad \forall A,B \subset S~,
$$
using $k(x,y)dy$ to represent the measure $K(x,dy)$.

\begin{pro}
  With respect to a measure $\pi$, detailed balance of a Markov operator $K$ implies invariance.
  \label{p:dbim}
\end{pro}
\begin{proof}
  Choose $A=S$ in \eqref{dbm}. The left hand side is $(\pi K)(B)$.
  On the right hand side $K(x,S)=1$ for all $x$ is the Markov property,
  leaving $\pi(B)$. This holds for any $B\in \cal S$.
\end{proof}


\subsection{M-H with proposals that are measures}

Tierney '98 and Andrieu prove that M-H with acceptance a generalization of
\eqref{al}
obeys DB in the measure sense; they are too abstract for our purposes.
% I don't get the analysis in Tierney '98, and certainly not Andrieu.
We now instead show the same in a simpler way.

We need M-H general measure proposal $Q(x,dy)$, which might not
be represented by an AC kernel $q(x,y)dy$ for any function $q(x,\cdot)$.
In this case the $\al$ condition \eqref{alrat} is meaningless and must be replaced by
\be
\int_{x\in A} \int_{y \in B} \pi(dx) Q(x,dy) \al(x,y) = \int_{x\in A} \int_{y\in B} \pi(dy) Q(y,dx) \al(y,x)~, \qquad \forall A,B \in \cal S~.
\label{alratm}
\ee
Note that the $dy$ notation is unavoidable because
$\al$ depends on $(x,y)$ and thus modifies $Q$ by pointwise multiplication;
one cannot use the set notation of \eqref{dbm}.
Here we take $\al$ as a function of $(x,y)$, which thus may safely multiply
measures pointwise in both variables.
% without Radon-Nykodim deriv, yuk
% $\al$ need only be defined on the support.
\eqref{alratm} can be summarized by an equality of
measures on $\cal S \otimes \cal S$,
\be
\pi(dx) Q(x,dy) \al(x,y) = \pi(dy) Q(y,dx) \al(y,x)~,
\label{alratmi}
\ee
in other words that the left hand side measure of \eqref{alratmi} is symmetric
with respect to $x\leftrightarrow y$.
%with respect to variables $(x,y)$.
% is this too loose to use?

The rejection probability is, for each $x\in S$,
\be
r(x) = 1 - \int Q(x,dz) \al(x,z)~,
\label{rejm}
\ee
and, in terms of this, the M-H transition measure (with respect to $y$) is
\be
K(x,dy) = Q(x,dy) \al(x,y) + r(x) \delta_x(dy)~,
\label{mhkerm}
\ee
or (Kennedy (8.7)),
\be
K(x,B) = \int_B Q(x,dy) \al(x,y) + r(x) 1_{x\in B}~, \qquad \forall B \in \cal S
~.
\label{mhkerB}
\ee

\begin{lem}
  For any acceptance probability function $\al(x,y)$ obeying \eqref{alratm},
  M-H has detailed balance with respect to the measure $\pi$.
\end{lem}
\begin{proof}
  Let $A,B\in \cal S$.
  Inserting \eqref{mhkerB} into the LHS of \eqref{dbm}
  gives
  $$
  \int_A \pi(dx) K(x,B) = \int_{x\in A} \pi(dx) \int_{y\in B} Q(x,dy) \al(x,y)
  + \int_{A\cap B} \pi(dx) r(x)
  $$
  Instead inserting into the RHS gives
  $$
  \int_B \pi(dx) K(x,A) = \int_{x\in B} \pi(dx) \int_{y\in A} Q(x,dy) \al(x,y)
  + \int_{A\cap B} \pi(dx) r(x)
  $$
  The second terms are equal.
  By \eqref{alratm} (swapping the roles $x\leftrightarrow y$) the first terms are equal,
  so \eqref{dbm} holds for all $A,B$.
\end{proof}

Combining with Prop.~\ref{p:dbim} shows that for general proposal measures,
M-H with \eqref{alratm} has the correct $\pi$-invariance.


\section{M-H with a deterministic proposal from a map}
% Q symm needed?

One component of Hamiltonian MC is an M-H step using a proposal measure defined by a map.
Let $F: S\to S$ be a deterministic smooth map (this will be given by, eg,
leapfrog steps).
Let the proposal measure $Q_F(x,\cdot)$ be defined by
$$
Q_F(x,B) = 1_{F(x)\in B} \qquad \forall B \in \cal S~,
$$
which simply places a unit point mass at $F(x)$.
This can also be written
$$
Q_F(x,dy) = \delta_{F(x)}(dy) = \delta(y - F(x)) dy ~.
$$
\begin{rmk} Acting this proposal directly as a Markov kernel (ie, always accepting) onto a measure $\pi$ gives $\pi Q_F$, the so-called ``pushforward'',
$$
(\pi Q_F)(B) = \int \pi(dx) Q_F(x,B) = \pi(F^{-1}(B))
$$
often denoted by $F_\ast \pi$, or by $\pi^F$ (Andrieu).
\end{rmk}

\begin{dfn}[Involution]
  A map $F:S\to S$ is an involution if $F^{-1} = F$,
  that is, $F^2=I$ where $I$ is the identity.
\end{dfn}
Andrieu, building on Tierney '98 p.4, emphasizes involutions,
although this is confused for this reader with the $x\leftrightarrow y$ flip
(see Andrieu eq.~(4)).

\begin{dfn}[Liouville]
  A map $F:S\to S$ is volume-preserving if it has unit Jacobean
  $|\det DF(x)| = 1$ for all $x\in S$.
\end{dfn}

\begin{lem}
  Consider $\pi$ an AC target density.
  Let $F$ be a volume-preserving involution.
  Then M-H with the proposal measure $Q_F$ and acceptance
  probability obeying
  \be
  \pi(x) \al(x,y) = \pi(y) \al(y,x)   \qquad \forall x,y, \in S
  \label{alrats}
  \ee
  has $\pi$ as an invariant measure.
\end{lem}
\begin{proof}


\end{proof}



\section{Hamiltonian Monte Carlo}

Here we use the above tools to prove that HMC has the correct invariant pdf.

The goal is to sample from a pdf $\pi(q)$ over a $d$-dimensional
Euclidean state space $q\in \R^d$. ($\pi$ is assumes AC as a measure.)
The MCMC state space is augmented to $x = (q,p)$ where $q\in R^d$ is now interpreted
as position and $p\in\R^d$ as momentum.
In the simplest case the Hamiltonian is
$$
H(q,p) = V(q) + \frac{1}{2}\|p\|^2
$$
where $V(q) = -\log \pi(q)$, that is, $\pi(q) = e^{-V(q)}$.
This assumes $\pi$ is strictly positive.

*** define leapfrog.

prove vol-pres.

time-reversible means if compose with $p$-flip, $F$ becomes involution.

HMC composes two steps:
ii) Gibbs sample (randomize) $p$.
i) M-H using map $F$ as some leapfrogs followed by $p$-flip.

thm HMC has $\pi$ invariant.




\section{Delayed rejection}

*** To do





% BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB
\bibliographystyle{abbrv}
\bibliography{localrefs}
\end{document}

